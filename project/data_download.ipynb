{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Downloading arxiv.zip to ../data\n",
      "100%|█████████████████████████████████████▉| 1.21G/1.21G [05:32<00:00, 4.64MB/s]\n",
      "100%|██████████████████████████████████████| 1.21G/1.21G [05:32<00:00, 3.91MB/s]\n"
     ]
    }
   ],
   "source": [
    "## this code should not be neccessary to run, it downloads the data from kaggle\n",
    "\n",
    "#%pip install -U -q kaggle\n",
    "#%mkdir -p ~/.kaggle\n",
    "#!echo '{\"username\":\"xxxx\",\"key\":\"xxxxxxxxx\"}' > ~/.kaggle/kaggle.json  # change to you own kaggle profile API token\n",
    "#!chmod 600 ~/.kaggle/kaggle.json\n",
    "#!kaggle datasets download -d Cornell-University/arxiv -p \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ../data/arxiv.zip\n",
      "  inflating: ../data/arxiv-metadata-oai-snapshot.json  \n"
     ]
    }
   ],
   "source": [
    "## you don't have to run this code too\n",
    "#!unzip -n '../data/arxiv.zip' -d '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Balázs', 'C.', ''], ['Berger', 'E. L.', ''], ['Nadolsky', 'P. M.', ''], ['Yuan', 'C. -P.', '']]\n",
      "[['Streinu', 'Ileana', ''], ['Theran', 'Louis', '']]\n",
      "[['Pan', 'Hongjun', '']]\n",
      "[['Callan', 'David', '']]\n",
      "[['Abu-Shammala', 'Wael', ''], ['Torchinsky', 'Alberto', '']]\n",
      "[['Pong', 'Y. H.', ''], ['Law', 'C. K.', '']]\n",
      "[['Corichi', 'Alejandro', ''], ['Vukasinac', 'Tatjana', ''], ['Zapata', 'Jose A.', '']]\n",
      "[['Swift', 'Damian C.', '']]\n",
      "[['Harvey', 'Paul', ''], ['Merin', 'Bruno', ''], ['Huard', 'Tracy L.', ''], ['Rebull', 'Luisa M.', ''], ['Chapman', 'Nicholas', ''], ['Evans', 'Neal J.', 'II'], ['Myers', 'Philip C.', '']]\n",
      "[['Ovchinnikov', 'Sergei', '']]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "## read in ALL the data, it may take a while\n",
    "#data = [json.loads(line)\n",
    "#        for line in open('data/arxiv-metadata-oai-snapshot.json', 'r', encoding='utf-8')]\n",
    "\n",
    "\n",
    "## use the following code to read in limited number of data\n",
    "def read_in_data(output_folder):\n",
    "    data = []\n",
    "    i = 0\n",
    "    for line in open('data/arxiv-metadata-oai-snapshot.json', 'r', encoding='utf-8', errors=\"replace\"):\n",
    "        data.append(json.loads(line))\n",
    "        i = i + 1\n",
    "        if i == 10:    # you can change the limit of reading in data objects by changing the number in if- condition\n",
    "            break\n",
    "    \n",
    "    ## if everything went well this line should print out the first element of the list\n",
    "    #print(json.dumps(data[0], indent=2))   \n",
    "    # data cleaning example: \"droping\" data with one word titles\n",
    "    data = [obj for obj in data if len(obj['title'].split(\" \")) > 1]\n",
    "    modified_data = []\n",
    "\n",
    "    ## line by line data modification\n",
    "    for line in data:\n",
    "        modified_data.append(line)\n",
    "        print(line.get(\"authors_parsed\"))\n",
    "\n",
    "    #with open(f'{output_folder}/cleaned_data.json', 'w') as json_file:\n",
    "    #    json.dump(modified_data, json_file, indent=2)\n",
    "    #return data     \n",
    "read_in_data(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0704.0027', '0704.0041', '0704.0044', '0704.0063', '0704.0064', '0704.0080', '0704.0098']\n"
     ]
    }
   ],
   "source": [
    "print([x['id'] for x in data if len(x['versions']) > 3])\n",
    "\n",
    "def clean_data(output_folder):\n",
    "\n",
    "    with open(f'{output_folder}/cleaned_data.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    # data cleaning example: \"droping\" data with one word titles\n",
    "    data = [obj for obj in data if len(obj['title'].split(\" \")) > 1]\n",
    "    modified_data = []\n",
    "\n",
    "    ## line by line data modification\n",
    "    for line in data:\n",
    "        modified_data.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#%pip install requests\n",
    "import requests\n",
    "\n",
    "def getArticleResource(output_folder):\n",
    "    \n",
    "    with open(f'{output_folder}/cleaned_data.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    #url = \"https://serpapi.com/search\"\n",
    "    url = \"https://dblp.org/search/publ/api\"\n",
    "\n",
    "    ##params = {\n",
    "    ##\"engine\": \"google_scholar\",\n",
    "    ##\"q\": title,\n",
    "    ##\"api_key\": \"95e72264387e5901953190dbc9608e0a4bc625ae3e2d4b3b3dca1c3995d628d8\"\n",
    "    ##}\n",
    "    for obj in data:\n",
    "        #print(obj.get(\"title\"))\n",
    "        #print(getArticleResource(obj.get(\"title\")))\n",
    "\n",
    "        params = {\n",
    "        \"q\": obj.get(\"title\"),\n",
    "        \"format\": \"json\",\n",
    "        \"h\": 20\n",
    "        }\n",
    "\n",
    "        response = requests.get(url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            results = response.json().get(\"result\").get(\"hits\")\n",
    "            if int(results.get(\"@total\")) == 0:\n",
    "                #return None\n",
    "                break\n",
    "            for hit in results.get(\"hit\"):\n",
    "                #print(hit.get(\"info\").get(\"url\"))\n",
    "                if hit.get(\"info\").get(\"title\") == obj.get(\"title\"):\n",
    "                    #return hit.get(\"info\").get(\"url\")\n",
    "                    obj['url'] = hit.get(\"info\").get(\"url\")\n",
    "            #organic_results = results.get(\"organic_results\")\n",
    "            #return organic_results\n",
    "            #return None\n",
    "    with open(f'{output_folder}/augmented_data.json', 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=2)\n",
    "\n",
    "#print(getArticleResource(\"Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies\"))\n",
    "#print(getArticleResource(\"test\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#for obj in modified_data:\n",
    "#    #print(obj.get(\"title\"))\n",
    "#    print(getArticleResource(obj.get(\"title\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG \n",
    "from airflow.operators.python_operator import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.bash_operator import BashOperator \n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "DEFAULT_ARGS = {\n",
    "    'owner': 'Tartu',\n",
    "    'depends_on_past': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "DATA_FOLDER = '/tmp/data'\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id='ingestion', # name of dag\n",
    "    schedule_interval='01 0 * * *', # execute every minute\n",
    "    start_date=datetime(2023,9,14,9,15,0),\n",
    "    catchup=False, # in case execution has been paused, should it execute everything in between\n",
    "    template_searchpath=DATA_FOLDER, # the PostgresOperator will look for files in this folder\n",
    "    default_args=DEFAULT_ARGS, # args assigned to all operators\n",
    ")\n",
    "\n",
    "file_sensor = FileSensor(\n",
    "    task_id='file_sensor',\n",
    "    filepath='/data/staging',\n",
    "    fs_conn_id='fs_default',\n",
    "    poke_interval=5,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "first_task = PythonOperator(\n",
    "    task_id='get_data',\n",
    "    dag=dag,\n",
    "    trigger_rule='none_failed',\n",
    "    python_callable=read_in_data,\n",
    "    op_kwargs={\n",
    "        'output_folder': DATA_FOLDER\n",
    "    },\n",
    ")\n",
    "\n",
    "#file_sensor >> first_task\n",
    "\n",
    "second_task = PythonOperator(\n",
    "    task_id='augment_data',\n",
    "    dag=dag,\n",
    "    trigger_rule='none_failed',\n",
    "    python_callable=getArticleResource,\n",
    "    op_kwargs={\n",
    "        'output_folder': DATA_FOLDER\n",
    "    },\n",
    ")\n",
    "\n",
    "first_task >> second_task\n",
    "\n",
    "def insert_categories(output_folder):\n",
    "    with open(f'{output_folder}/augmented_data.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    sql_file = ''\n",
    "    unique_cat = set()\n",
    "\n",
    "    for row in data:\n",
    "        if row.get(\"categories\") not in unique_cat:\n",
    "            unique_cat.add(row.get(\"categories\"))\n",
    "    \n",
    "    #article_id = 1\n",
    "    cat_id = 1\n",
    "    #author_id = 1\n",
    "    #article_author_id = 1\n",
    "    for cat in unique_cat:\n",
    "        sql_file = sql_file + f'INSERT INTO category (ID, name) VALUES (\\'{cat_id}\\', \\'{cat}\\');\\n'\n",
    "        cat_id += 1\n",
    "    \n",
    "    with open(f'{output_folder}/insert_categories.sql', 'w') as f:\n",
    "        f.write(sql_file)\n",
    "\n",
    "\n",
    "def insert_authors(output_folder):\n",
    "    with open(f'{output_folder}/augmented_data.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    sql_file = ''\n",
    "    unique_authors = set()\n",
    "\n",
    "    for row in data:\n",
    "        if len(row.get(\"authors\").split(\",\")) == 0:\n",
    "            if author.strip() not in unique_authors:\n",
    "                unique_authors.add(author.strip())\n",
    "        else:\n",
    "            authors = row.get(\"authors\").split(\",\")\n",
    "            for author in authors:\n",
    "                if author.strip() not in unique_authors:\n",
    "                    unique_authors.add(author.strip())\n",
    "    \n",
    "    #article_id = 1\n",
    "    author_id = 1\n",
    "    #article_author_id = 1\n",
    "    for author in unique_authors:\n",
    "        if len(author.split(\" \")) == 0:\n",
    "            names_separated = [\"\", author]\n",
    "        else:\n",
    "            names_separated = author.rsplit(\" \", 1)\n",
    "        sql_file = sql_file + f'INSERT INTO author (ID, first_name, last_name) VALUES (\\'{author_id}\\', \\'{names_separated[0]}\\', \\'{names_separated[1]}\\');\\n'\n",
    "        author_id += 1\n",
    "    \n",
    "    with open(f'{output_folder}/insert_authors.sql', 'w') as f:\n",
    "        f.write(sql_file)\n",
    "\n",
    "\n",
    "def insert_articles(output_folder):\n",
    "    with open(f'{output_folder}/augmented_data.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    sql_file = ''\n",
    "    \n",
    "    for row in data:\n",
    "        sql_file = sql_file + f'INSERT INTO article (ID, title, doi, update_date, journal_ref, category_id) \n",
    "        VALUES (\\'{row.get(\"id\")}\\', \\'{row.get(\"title\")}\\',\\'{row.get(\"doi\")}\\',\\'{row.get(\"update_date\")}\\',\\'{row.get(\"journal-ref\")}\\',\n",
    "        (SELECT ID FROM category WHERE name = \\'{row.get(\"categories\")}\\'));\\n'\n",
    "    \n",
    "    with open(f'{output_folder}/insert_articles.sql', 'w') as f:\n",
    "        f.write(sql_file)\n",
    "\n",
    "\n",
    "def insert_articles_authors(output_folder):\n",
    "    with open(f'{output_folder}/augmented_data.json', 'r') as f:\n",
    "        data = json.load(f)\n",
    "    sql_file = ''\n",
    "    \n",
    "    article_author_id = 1\n",
    "    for row in data:\n",
    "        if len(row.get(\"authors\").split(\",\")) == 0:\n",
    "            authors = [row.get(\"authors\").split(\",\")]\n",
    "        else:\n",
    "            authors = row.get(\"authors\").split(\",\")\n",
    "        for author in authors:\n",
    "            if len(author.split(\" \")) == 0:\n",
    "                names_separated = [\"\", author]\n",
    "            else:\n",
    "                names_separated = author.rsplit(\" \", 1)\n",
    "            sql_file = sql_file + f'INSERT INTO article_author (ID, article_id, author_id) \n",
    "            VALUES (\\'{article_author_id}\\', \\'{row.get(\"id\")}\\', (SELECT ID FROM author WHERE first_name = \\'{names_separated[0]}\\' AND last_name=\\'{names_separated[1]}\\'));\\n'\n",
    "                \n",
    "    with open(f'{output_folder}/insert_article_author.sql', 'w') as f:\n",
    "        f.write(sql_file)\n",
    "\n",
    "\n",
    "create_tables = PostgresOperator(\n",
    "    task_id=\"create_tables\",\n",
    "    postgres_conn_id=\"postgres_default\",\n",
    "    sql=\"sql/create_tables.sql\",\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "second_task >> create_tables\n",
    "\n",
    "third_task = PythonOperator(\n",
    "    task_id='insert_categories',\n",
    "    dag=dag,\n",
    "    trigger_rule='none_failed',\n",
    "    python_callable=insert_categories,\n",
    "    op_kwargs={\n",
    "        'output_folder': DATA_FOLDER,\n",
    "    },\n",
    ")\n",
    "\n",
    "create_tables >> third_task\n",
    "\n",
    "fourth_task = PostgresOperator(\n",
    "    task_id='insert_categories_to_db',\n",
    "    dag=dag,\n",
    "    postgres_conn_id='airflow_pg',\n",
    "    sql='insert_categories.sql',\n",
    "    trigger_rule='none_failed',\n",
    "    autocommit=True,\n",
    ")\n",
    "\n",
    "third_task >> fourth_task\n",
    "\n",
    "fifth_task = PythonOperator(\n",
    "    task_id='insert_authors',\n",
    "    dag=dag,\n",
    "    trigger_rule='none_failed',\n",
    "    python_callable=insert_categories,\n",
    "    op_kwargs={\n",
    "        'output_folder': DATA_FOLDER,\n",
    "    },\n",
    ")\n",
    "\n",
    "fourth_task >> fifth_task\n",
    "\n",
    "sixth_task = PostgresOperator(\n",
    "    task_id='insert_authors_to_db',\n",
    "    dag=dag,\n",
    "    postgres_conn_id='airflow_pg',\n",
    "    sql='insert_authors.sql',\n",
    "    trigger_rule='none_failed',\n",
    "    autocommit=True,\n",
    ")\n",
    "\n",
    "fifth_task >> sixth_task\n",
    "\n",
    "seventh_task = PythonOperator(\n",
    "    task_id='insert_articles',\n",
    "    dag=dag,\n",
    "    trigger_rule='none_failed',\n",
    "    python_callable=insert_articles,\n",
    "    op_kwargs={\n",
    "        'output_folder': DATA_FOLDER,\n",
    "    },\n",
    ")\n",
    "\n",
    "sixth_task >> seventh_task\n",
    "\n",
    "eightth_task = PostgresOperator(\n",
    "    task_id='insert_articles_to_db',\n",
    "    dag=dag,\n",
    "    postgres_conn_id='airflow_pg',\n",
    "    sql='insert_articles.sql',\n",
    "    trigger_rule='none_failed',\n",
    "    autocommit=True,\n",
    ")\n",
    "\n",
    "seventh_task >> eightth_task\n",
    "\n",
    "ninth_task = PythonOperator(\n",
    "    task_id='insert_article_author',\n",
    "    dag=dag,\n",
    "    trigger_rule='none_failed',\n",
    "    python_callable=insert_articles_authors,\n",
    "    op_kwargs={\n",
    "        'output_folder': DATA_FOLDER,\n",
    "    },\n",
    ")\n",
    "\n",
    "eightth_task >> ninth_task\n",
    "\n",
    "tenth_task = PostgresOperator(\n",
    "    task_id='insert_article_authors_to_db',\n",
    "    dag=dag,\n",
    "    postgres_conn_id='airflow_pg',\n",
    "    sql='insert_article_author.sql',\n",
    "    trigger_rule='none_failed',\n",
    "    autocommit=True,\n",
    ")\n",
    "\n",
    "ninth_task >> tenth_task\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
